# Quantifying Latency-Accuracy of LLMs

LLMs are being used for a variety of tasks involving physical sensing and actions in settings where delay matters for user experience and application performance. There are myriad models of different sizes, some running on cloud vs some on the edge.

This project presents a comprehensive analysis of deploying various **Large Language Models (LLMs)**, including GPT, Llama, and other fine-tuned and open-source LLMs, across different computing environmentsâ€”edge, cloud, and server platforms. The core objective is to assess the adaptability and efficiency of these advanced models in diverse operational contexts. We have meticulously designed a suite of tasks(i.e., text classification, machine translation, and mathematics solving) to quantify the performance and accuracy of LLMs in each environment(including Cloud services, Mac with M2 Chip and GPU server). Our measurement result has the following findings:

1. Accuracy didn't differ a lot on different platforms
2. Latency is dependent by different deployment platform
3. Different tasks also have an effect on latency
4. Lightweight deployment on LLM across different edge platforms with low latency remains a challenge.

# Team

* Name of team member \#1 : Zhuohao Li
* Name of team member \#2 : Ying Li

# Required Submissions

* [Proposal](proposal)
* [Midterm Checkpoint Presentation Slides](https://bu9gy.github.io/midterm_slides.pdf)
* [Final Presentation Slides](http://)
* [Final Report](report)
